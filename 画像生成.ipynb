{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxEwhlFegdt2SAXYWtUf+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maeda0622/machine_learning/blob/main/%E7%94%BB%E5%83%8F%E7%94%9F%E6%88%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "gsn26Mxrznsx",
        "outputId": "511d1545-0650-4a44-ca0e-206ba0e137d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-511fb91b8d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m  \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-511fb91b8d1a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m                            \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColorJitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                            \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                            \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                        ])) \n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/User/Downloads/img_align_celeba/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "# Initial_setting\n",
        "workers = 1\n",
        "batch_size=64\n",
        "nz = 100\n",
        "nch_g = 64\n",
        "nch_d = 64\n",
        "n_epoch = 10000\n",
        "lr = 0.001\n",
        "beta1 = 0.5\n",
        "outf = './result_lsgan'\n",
        "display_interval = 100\n",
        "save_fake_image_interval = 1500\n",
        "plt.rcParams['figure.figsize'] = 10, 6\n",
        " \n",
        " \n",
        "try:\n",
        " os.makedirs(outf, exist_ok=True)\n",
        "except OSError as error:\n",
        " print(error)\n",
        " pass\n",
        " \n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        " \n",
        " \n",
        "def weights_init(m):\n",
        " classname = m.__class__.__name__\n",
        " if classname.find('Conv') != -1:         \n",
        "     m.weight.data.normal_(0.0, 0.02)\n",
        "     m.bias.data.fill_(0)\n",
        " elif classname.find('Linear') != -1:     \n",
        "     m.weight.data.normal_(0.0, 0.02)\n",
        "     m.bias.data.fill_(0)\n",
        " elif classname.find('BatchNorm') != -1: \n",
        "     m.weight.data.normal_(1.0, 0.02)\n",
        "     m.bias.data.fill_(0)\n",
        " \n",
        " \n",
        "class Generator(nn.Module):\n",
        " def __init__(self, nz=100, nch_g=64, nch=3):\n",
        "     super(Generator, self).__init__()\n",
        "     self.layers = nn.ModuleDict({\n",
        "         'layer0': nn.Sequential(\n",
        "             nn.ConvTranspose2d(nz, nch_g * 8, 4, 1, 0), \n",
        "             nn.BatchNorm2d(nch_g * 8),                   \n",
        "             nn.ReLU()                                 \n",
        "         ),  # (100, 1, 1) -> (512, 4, 4)\n",
        "         'layer1': nn.Sequential(\n",
        "             nn.ConvTranspose2d(nch_g * 8, nch_g * 4, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_g * 4),\n",
        "             nn.ReLU() \n",
        "         ),  # (512, 4, 4) -> (256, 8, 8)\n",
        "         'layer2': nn.Sequential(\n",
        "             nn.ConvTranspose2d(nch_g * 4, nch_g * 2, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_g * 2),\n",
        "             nn.ReLU()  \n",
        "         ),  # (256, 8, 8) -> (128, 16, 16)\n",
        " \n",
        "         'layer3': nn.Sequential(\n",
        "             nn.ConvTranspose2d(nch_g * 2, nch_g, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_g),\n",
        "             nn.ReLU() \n",
        "         ),  # (128, 16, 16) -> (64, 32, 32)\n",
        "         'layer4': nn.Sequential(\n",
        "             nn.ConvTranspose2d(nch_g, nch, 4, 2, 1),\n",
        "             nn.Tanh()\n",
        "         )   # (64, 32, 32) -> (3, 64, 64)\n",
        "     })\n",
        " \n",
        " def forward(self, z):\n",
        "     for layer in self.layers.values(): \n",
        "         z = layer(z)\n",
        "     return z\n",
        " \n",
        " \n",
        "class Discriminator(nn.Module):\n",
        " def __init__(self, nch=3, nch_d=64):\n",
        "     super(Discriminator, self).__init__()\n",
        "     self.layers = nn.ModuleDict({\n",
        "         'layer0': nn.Sequential(\n",
        "             nn.Conv2d(nch, nch_d, 4, 2, 1), \n",
        "             nn.BatchNorm2d(nch_d),\n",
        "             nn.LeakyReLU(negative_slope=0.2) \n",
        "         ),  # (3, 64, 64) -> (64, 32, 32)\n",
        "         'layer1': nn.Sequential(\n",
        "             nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_d*2),\n",
        "             nn.LeakyReLU(negative_slope=0.2)\n",
        "         ),  # (64, 32, 32) -> (128, 16, 16)\n",
        "         'layer2': nn.Sequential(\n",
        "             nn.Conv2d(nch_d * 2, nch_d * 4, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_d*4),\n",
        "             nn.LeakyReLU(negative_slope=0.2)\n",
        "         ),  # (128, 16, 16) -> (256, 8, 8)\n",
        "         'layer3': nn.Sequential(\n",
        "             nn.Conv2d(nch_d * 4, nch_d * 8, 4, 2, 1),\n",
        "             nn.BatchNorm2d(nch_d*8),\n",
        "             nn.LeakyReLU(negative_slope=0.2)\n",
        "         ),  # (256, 8, 8) -> (512, 4, 4)\n",
        "         'layer4':nn.Sequential( nn.Conv2d(nch_d * 8, 1, 4, 1, 0)\n",
        "         # (512, 4, 4) -> (1, 1, 1)\n",
        "         #勾配消失を防ぐためにSigmoidは使わない\n",
        "         )\n",
        "     })\n",
        " \n",
        " def forward(self, x):\n",
        "     for layer in self.layers.values(): \n",
        "         x = layer(x)\n",
        "         x = x.squeeze()\n",
        "       \n",
        "     return x\n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "def main():\n",
        " #パスには入っている画像データセットの一個上の改装を\n",
        " #例:もし  your_home/Face_Datasets/Japanese/000.jpgのような階層になっていれば\n",
        " #root = your_home/Face_Datasetsとする\n",
        " dataset = dset.ImageFolder(root='C:/Users/User/Downloads/img_align_celeba/',\n",
        "                           transform=transforms.Compose([\n",
        "                           transforms.RandomResizedCrop(64, scale=(1.0, 1.0), ratio=(1., 1.)),\n",
        "                           transforms.RandomHorizontalFlip(),\n",
        "                           transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                       ])) \n",
        "  \n",
        " dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                     shuffle=True, num_workers=int(workers))\n",
        " \n",
        " #GPUがあるならGPUデバイスを作動\n",
        " device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        " \n",
        " #Generatorを定義\n",
        " netG = Generator(nz=nz, nch_g=nch_g).to(device)\n",
        " #ネットワークパラメ-タをランダムに初期化\n",
        " netG.apply(weights_init) \n",
        "  \n",
        " #Discriminatorを定義\n",
        " netD = Discriminator(nch_d=nch_d).to(device)\n",
        " \n",
        " #ネットワークパラメ-タをランダムに初期化\n",
        " netD.apply(weights_init)\n",
        " \n",
        " #損失関数を二乗誤差に設定\n",
        " criterion = nn.MSELoss()  \n",
        " \n",
        " #それそれのパラメータ更新用のoptimizerを定義\n",
        " optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5) \n",
        " optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5) \n",
        " \n",
        " Loss_D_list, Loss_G_list = [], []\n",
        " \n",
        " fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)  # save_fake_image用ノイズ（固定）\n",
        " for epoch in range(n_epoch):\n",
        "     \n",
        "     for itr, data in enumerate(dataloader):\n",
        "         \n",
        "         \n",
        "         real_image = data[0].to(device)   # 本物画像\n",
        "         \n",
        "         sample_size = real_image.size(0)  # 画像枚数\n",
        "         noise = torch.randn(sample_size, nz, 1, 1, device=device)   # 入力ベクトル生成（正規分布ノイズ）   \n",
        "         real_target = torch.full((sample_size,), random.uniform(1, 1), device=device)   # 本物ラベル\n",
        "         fake_target = torch.full((sample_size,), random.uniform(0, 0), device=device)   # 偽物ラベル\n",
        "         \n",
        "       \n",
        "         #Generator→Discriminatorの順にパラメータ更新\n",
        "         \n",
        "         #---------  Update Generator   ----------\n",
        "         netG.zero_grad()     \n",
        "         fake_image = netG(noise) # Generatorから得られた偽画像\n",
        "         output = netD(fake_image)   # 更新した Discriminatorで、偽物画像を判定\n",
        "         errG = criterion(output,real_target)   # 偽物画像の判定結果と本物ラベルとの二乗誤差\n",
        "         errG.backward(retain_graph = True) # 誤差逆伝播\n",
        "         D_G_z2 = output.mean().item()  # outputの平均 D_G_z2 を計算（後でログ出力に使用）\n",
        " \n",
        "         optimizerG.step()  # Generatorのパラメータ更新\n",
        "         \n",
        "         #---------  Update Discriminaator   ----------\n",
        "         netD.zero_grad() # 勾配の初期化  \n",
        "         fake_image = netG(noise) # Generatorから得られた偽画像\n",
        "         output = netD(real_image)   # Discriminatorが行った、本物画像の判定結果\n",
        "         errD_real = criterion(output,real_target)  # 本物画像の判定結果と本物ラベルとの二乗誤差\n",
        "         D_x = output.mean().item()  # outputの平均 D_x を計算（後でログ出力に使用）\n",
        "     \n",
        "         output = netD(fake_image.detach())  # Discriminatorが行った、偽物画像の判定結果\n",
        "         \n",
        "         errD_fake = criterion(output,fake_target)  # 偽物画像の判定結果と偽物画像との二乗誤差\n",
        "         D_G_z1 = output.mean().item()  # outputの平均 D_G_z1 を計算（後でログ出力に使用）\n",
        " \n",
        "         errD =  errD_real + errD_fake # Discriminator 全体の損失\n",
        "         errD.backward(retain_graph = True) # 誤差逆伝播\n",
        "         optimizerD.step()   # Discriminatoeのパラメーター更新\n",
        "         \n",
        "           \n",
        "                 \n",
        "         \n",
        "         \n",
        " \n",
        "         #定期的に損失を表示\n",
        "         if itr % 5 == 0:\n",
        "             print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
        "                   .format(epoch + 1, n_epoch,\n",
        "                           itr + 1, len(dataloader),\n",
        "                           errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "             \n",
        "             Loss_D_list.append(errD.item())\n",
        "             Loss_G_list.append(errG.item())\n",
        "                           \n",
        "         #定期的に画像を保存\n",
        "         if (itr + 1) % 50 == 0:\n",
        "             \n",
        "             fake_image = netG(fixed_noise) \n",
        "             vutils.save_image(fake_image.detach(), './GAN/{:03d}random_{:03d}.png'.format(itr,epoch + 1),\n",
        "                               normalize=True, nrow=8)\n",
        " \n",
        "     \n",
        "     \n",
        " \n",
        " # plot graph\n",
        " plt.figure() \n",
        " plt.plot(range(len(Loss_D_list)), Loss_D_list, color='blue', linestyle='-', label='Loss_D')\n",
        " plt.plot(range(len(Loss_G_list)), Loss_G_list, color='red', linestyle='-', label='Loss_G')\n",
        " plt.legend()\n",
        " plt.xlabel('iter (*100)')\n",
        " plt.ylabel('loss')\n",
        " plt.title('Loss_D and Loss_G')\n",
        " plt.grid()\n",
        " plt.savefig('Loss_graph.png') \n",
        " \n",
        "if __name__ == '__main__':\n",
        " main()"
      ]
    }
  ]
}